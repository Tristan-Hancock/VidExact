{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mffmpeg\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import wave\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ffmpeg\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import torch  # For YOLO object detection\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import InputLayer as KerasInputLayer  # For custom layer\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "\n",
    "import vosk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import wave\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ffmpeg\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import torch  # For YOLO object detection\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import InputLayer as KerasInputLayer  # For custom layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustom3DCNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Custom3DCNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vgg_face_descriptor = load_model('../models/vgg_face_descriptor.h5')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "pca = joblib.load('../models/pca.pkl')\n",
    "clf = joblib.load('../models/svm_classifier.pkl')\n",
    "le = joblib.load('../models/label_encoder.pkl')\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)\n",
    "\n",
    "# -------------------- Action Detection Function --------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Pipeline Functions --------------------\n",
    "# Function 1: Detect Scenes using scenedetect\n",
    "def detect_scenes(video_path, content_threshold=50.0, threshold_val=20, min_scene_len=15):\n",
    "    from scenedetect import open_video, SceneManager\n",
    "    from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "\n",
    "    video = open_video(video_path)\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=content_threshold))\n",
    "    scene_manager.add_detector(ThresholdDetector(threshold=threshold_val, min_scene_len=min_scene_len))\n",
    "    scene_manager.detect_scenes(video=video)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    print(f\"[LOG] detect_scenes: Detected {len(scene_list)} scene(s) in '{video_path}'.\")\n",
    "    return scene_list\n",
    "\n",
    "# Function 2: Trim Video Clip using ffmpeg-python\n",
    "def trim_clip(input_video, start_time, end_time, clip_index, max_duration=4.0, output_dir='clips'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    duration = end_time - start_time\n",
    "    if duration > max_duration:\n",
    "        end_time = start_time + max_duration  # Truncate to max_duration\n",
    "    clip_name = f\"uploaded_video_{clip_index:03d}.mp4\"\n",
    "    output_path = os.path.join(output_dir, clip_name)\n",
    "    \n",
    "    try:\n",
    "        ffmpeg.input(input_video, ss=start_time, t=(end_time - start_time)) \\\n",
    "              .output(output_path, vcodec='libx264', acodec='aac') \\\n",
    "              .run(overwrite_output=True)\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error trimming clip:\", e)\n",
    "    \n",
    "    print(f\"[LOG] trim_clip: Trimmed clip {clip_name} from {start_time}s to {end_time}s (max {max_duration}s).\")\n",
    "    return output_path\n",
    "\n",
    "# Function 3: Extract Key Frame using OpenCV\n",
    "def extract_key_frame(clip_path, frame_index='middle', output_dir='frames'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(clip_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "    \n",
    "    # Determine target time: use the middle of the clip by default\n",
    "    target_time = duration / 2 if frame_index == 'middle' else 0\n",
    "    target_frame = int(target_time * fps)\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_file_name = os.path.basename(clip_path).split('.')[0] + \"_keyframe.jpg\"\n",
    "        frame_path = os.path.join(output_dir, frame_file_name)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        print(f\"[LOG] extract_key_frame: Extracted key frame at {target_time:.2f}s -> {frame_path}\")\n",
    "        cap.release()\n",
    "        return frame_path\n",
    "    else:\n",
    "        cap.release()\n",
    "        print(\"[LOG] extract_key_frame: Failed to extract frame.\")\n",
    "        return None\n",
    "\n",
    "# Function 4: Speech-to-Text using Vosk (with ffmpeg for audio extraction)\n",
    "def speech_to_text(clip_path, model_path=\"model\"):\n",
    "    temp_audio = \"temp_audio.wav\"\n",
    "    \n",
    "    try:\n",
    "        ffmpeg.input(clip_path).output(temp_audio, ac=1, ar='16k').run(overwrite_output=True)\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error extracting audio:\", e)\n",
    "        return \"\"\n",
    "    \n",
    "    wf = wave.open(temp_audio, \"rb\")\n",
    "    print(\"[LOG] speech_to_text: Checking audio format...\")\n",
    "    print(\"    Channels:\", wf.getnchannels())\n",
    "    print(\"    Sample Width:\", wf.getsampwidth())\n",
    "    print(\"    Frame Rate:\", wf.getframerate())\n",
    "    \n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:\n",
    "        wf.close()\n",
    "        os.remove(temp_audio)\n",
    "        raise ValueError(\"Audio must be WAV mono PCM at 16kHz.\")\n",
    "    \n",
    "    import vosk\n",
    "    model_instance = vosk.Model(lang=\"en-us\") if model_path == \"model\" else vosk.Model(model_path)\n",
    "    rec = vosk.KaldiRecognizer(model_instance, wf.getframerate())\n",
    "    \n",
    "    results = []\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            result_json = json.loads(rec.Result())\n",
    "            results.append(result_json)\n",
    "    \n",
    "    final_result = json.loads(rec.FinalResult())\n",
    "    results.append(final_result)\n",
    "    wf.close()\n",
    "    os.remove(temp_audio)\n",
    "    \n",
    "    recognized_texts = [r[\"text\"] for r in results if \"text\" in r]\n",
    "    full_text = \" \".join(recognized_texts)\n",
    "    print(f\"[LOG] speech_to_text: Transcribed {len(full_text.split())} word(s).\")\n",
    "    return full_text\n",
    "\n",
    "# Function 5: Face Detection & Recognition using MediaPipe\n",
    "def recognize_face_mediapipe(frame_path):\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "    \n",
    "    image = cv2.imread(frame_path)\n",
    "    if image is None:\n",
    "        print(\"[LOG] recognize_face_mediapipe: Could not read image from\", frame_path)\n",
    "        return \"NoFaceDetected\"\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image_rgb)\n",
    "    \n",
    "    if not results.detections:\n",
    "        print(\"[LOG] recognize_face_mediapipe: No face detected in the frame.\")\n",
    "        return \"NoFaceDetected\"\n",
    "    \n",
    "    detection = results.detections[0]\n",
    "    bboxC = detection.location_data.relative_bounding_box\n",
    "    ih, iw, _ = image.shape\n",
    "    x_min = max(0, int(bboxC.xmin * iw))\n",
    "    y_min = max(0, int(bboxC.ymin * ih))\n",
    "    width = int(bboxC.width * iw)\n",
    "    height = int(bboxC.height * ih)\n",
    "    face_snippet = image[y_min:y_min+height, x_min:x_min+width]\n",
    "    \n",
    "    identity = recognize_face_from_snippet(face_snippet)\n",
    "    return identity\n",
    "\n",
    "def recognize_face_from_snippet(face_snippet):\n",
    "    global vgg_face_descriptor, scaler, pca, clf, le\n",
    "    img = cv2.cvtColor(face_snippet, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img_expanded = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    embedding_vector = vgg_face_descriptor.predict(img_expanded)[0]\n",
    "    embedding_vector = scaler.transform([embedding_vector])\n",
    "    embedding_vector = pca.transform(embedding_vector)\n",
    "    y_pred = clf.predict(embedding_vector)\n",
    "    predicted_name = le.inverse_transform(y_pred)[0]\n",
    "    return predicted_name\n",
    "\n",
    "# Function 6: Object Detection using YOLOv5\n",
    "def detect_objects_yolo(image_path, conf_threshold=0.5):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"[LOG] detect_objects_yolo: Could not read image from {image_path}\")\n",
    "        return []\n",
    "    \n",
    "    results = yolo_model(image)\n",
    "    df = results.pandas().xyxy[0]\n",
    "    df = df[df['confidence'] >= conf_threshold]\n",
    "    \n",
    "    img_area = image.shape[0] * image.shape[1]\n",
    "    detections = []\n",
    "    for index, row in df.iterrows():\n",
    "        x1, y1, x2, y2 = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "        bbox_area = (x2 - x1) * (y2 - y1)\n",
    "        prominence = bbox_area / img_area\n",
    "        detections.append({\n",
    "            \"class\": row['name'],\n",
    "            \"confidence\": float(row['confidence']),\n",
    "            \"prominence\": prominence\n",
    "        })\n",
    "    \n",
    "    print(f\"[LOG] detect_objects_yolo: Found {len(detections)} high-confidence objects.\")\n",
    "    return detections\n",
    "\n",
    "def recognize_text(image_path):\n",
    "    try:\n",
    "        # Open the image using PIL and run pytesseract OCR\n",
    "        img = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        print(f\"[LOG] recognize_text: OCR found {len(text.split())} words.\")\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"[LOG] recognize_text: Error processing OCR:\", e)\n",
    "        return \"\"\n",
    "    \n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting main pipeline process...\n",
      "[LOG] process_video: Starting processing for '../input/video.mp4'...\n",
      "[LOG] detect_scenes: Detected 83 scene(s) in '../input/video.mp4'.\n",
      "[DEBUG] process_video: Scenes -> [(00:00:00.000 [frame=0, fps=30.000], 00:00:03.133 [frame=94, fps=30.000]), (00:00:03.133 [frame=94, fps=30.000], 00:00:05.533 [frame=166, fps=30.000]), (00:00:05.533 [frame=166, fps=30.000], 00:00:07.633 [frame=229, fps=30.000]), (00:00:07.633 [frame=229, fps=30.000], 00:00:08.333 [frame=250, fps=30.000]), (00:00:08.333 [frame=250, fps=30.000], 00:00:10.633 [frame=319, fps=30.000]), (00:00:10.633 [frame=319, fps=30.000], 00:00:13.833 [frame=415, fps=30.000]), (00:00:13.833 [frame=415, fps=30.000], 00:00:19.400 [frame=582, fps=30.000]), (00:00:19.400 [frame=582, fps=30.000], 00:00:22.933 [frame=688, fps=30.000]), (00:00:22.933 [frame=688, fps=30.000], 00:00:26.433 [frame=793, fps=30.000]), (00:00:26.433 [frame=793, fps=30.000], 00:00:30.833 [frame=925, fps=30.000]), (00:00:30.833 [frame=925, fps=30.000], 00:00:33.933 [frame=1018, fps=30.000]), (00:00:33.933 [frame=1018, fps=30.000], 00:00:35.533 [frame=1066, fps=30.000]), (00:00:35.533 [frame=1066, fps=30.000], 00:00:36.033 [frame=1081, fps=30.000]), (00:00:36.033 [frame=1081, fps=30.000], 00:00:39.033 [frame=1171, fps=30.000]), (00:00:39.033 [frame=1171, fps=30.000], 00:00:39.533 [frame=1186, fps=30.000]), (00:00:39.533 [frame=1186, fps=30.000], 00:00:40.033 [frame=1201, fps=30.000]), (00:00:40.033 [frame=1201, fps=30.000], 00:00:40.533 [frame=1216, fps=30.000]), (00:00:40.533 [frame=1216, fps=30.000], 00:00:41.033 [frame=1231, fps=30.000]), (00:00:41.033 [frame=1231, fps=30.000], 00:00:41.533 [frame=1246, fps=30.000]), (00:00:41.533 [frame=1246, fps=30.000], 00:00:42.033 [frame=1261, fps=30.000]), (00:00:42.033 [frame=1261, fps=30.000], 00:00:42.533 [frame=1276, fps=30.000]), (00:00:42.533 [frame=1276, fps=30.000], 00:00:43.033 [frame=1291, fps=30.000]), (00:00:43.033 [frame=1291, fps=30.000], 00:00:44.333 [frame=1330, fps=30.000]), (00:00:44.333 [frame=1330, fps=30.000], 00:00:47.967 [frame=1439, fps=30.000]), (00:00:47.967 [frame=1439, fps=30.000], 00:00:50.833 [frame=1525, fps=30.000]), (00:00:50.833 [frame=1525, fps=30.000], 00:00:53.433 [frame=1603, fps=30.000]), (00:00:53.433 [frame=1603, fps=30.000], 00:00:56.333 [frame=1690, fps=30.000]), (00:00:56.333 [frame=1690, fps=30.000], 00:00:57.833 [frame=1735, fps=30.000]), (00:00:57.833 [frame=1735, fps=30.000], 00:00:58.633 [frame=1759, fps=30.000]), (00:00:58.633 [frame=1759, fps=30.000], 00:01:15.733 [frame=2272, fps=30.000]), (00:01:15.733 [frame=2272, fps=30.000], 00:01:20.500 [frame=2415, fps=30.000]), (00:01:20.500 [frame=2415, fps=30.000], 00:01:23.533 [frame=2506, fps=30.000]), (00:01:23.533 [frame=2506, fps=30.000], 00:01:24.233 [frame=2527, fps=30.000]), (00:01:24.233 [frame=2527, fps=30.000], 00:01:26.033 [frame=2581, fps=30.000]), (00:01:26.033 [frame=2581, fps=30.000], 00:01:27.033 [frame=2611, fps=30.000]), (00:01:27.033 [frame=2611, fps=30.000], 00:01:27.533 [frame=2626, fps=30.000]), (00:01:27.533 [frame=2626, fps=30.000], 00:01:28.633 [frame=2659, fps=30.000]), (00:01:28.633 [frame=2659, fps=30.000], 00:01:30.667 [frame=2720, fps=30.000]), (00:01:30.667 [frame=2720, fps=30.000], 00:01:31.333 [frame=2740, fps=30.000]), (00:01:31.333 [frame=2740, fps=30.000], 00:01:31.833 [frame=2755, fps=30.000]), (00:01:31.833 [frame=2755, fps=30.000], 00:01:32.333 [frame=2770, fps=30.000]), (00:01:32.333 [frame=2770, fps=30.000], 00:01:38.700 [frame=2961, fps=30.000]), (00:01:38.700 [frame=2961, fps=30.000], 00:01:39.767 [frame=2993, fps=30.000]), (00:01:39.767 [frame=2993, fps=30.000], 00:01:46.167 [frame=3185, fps=30.000]), (00:01:46.167 [frame=3185, fps=30.000], 00:01:48.433 [frame=3253, fps=30.000]), (00:01:48.433 [frame=3253, fps=30.000], 00:01:51.500 [frame=3345, fps=30.000]), (00:01:51.500 [frame=3345, fps=30.000], 00:01:52.967 [frame=3389, fps=30.000]), (00:01:52.967 [frame=3389, fps=30.000], 00:01:58.633 [frame=3559, fps=30.000]), (00:01:58.633 [frame=3559, fps=30.000], 00:01:59.667 [frame=3590, fps=30.000]), (00:01:59.667 [frame=3590, fps=30.000], 00:02:02.233 [frame=3667, fps=30.000]), (00:02:02.233 [frame=3667, fps=30.000], 00:02:02.933 [frame=3688, fps=30.000]), (00:02:02.933 [frame=3688, fps=30.000], 00:02:05.233 [frame=3757, fps=30.000]), (00:02:05.233 [frame=3757, fps=30.000], 00:02:06.667 [frame=3800, fps=30.000]), (00:02:06.667 [frame=3800, fps=30.000], 00:02:08.100 [frame=3843, fps=30.000]), (00:02:08.100 [frame=3843, fps=30.000], 00:02:09.667 [frame=3890, fps=30.000]), (00:02:09.667 [frame=3890, fps=30.000], 00:02:11.333 [frame=3940, fps=30.000]), (00:02:11.333 [frame=3940, fps=30.000], 00:02:14.133 [frame=4024, fps=30.000]), (00:02:14.133 [frame=4024, fps=30.000], 00:02:16.400 [frame=4092, fps=30.000]), (00:02:16.400 [frame=4092, fps=30.000], 00:02:18.867 [frame=4166, fps=30.000]), (00:02:18.867 [frame=4166, fps=30.000], 00:02:25.333 [frame=4360, fps=30.000]), (00:02:25.333 [frame=4360, fps=30.000], 00:02:26.233 [frame=4387, fps=30.000]), (00:02:26.233 [frame=4387, fps=30.000], 00:02:27.567 [frame=4427, fps=30.000]), (00:02:27.567 [frame=4427, fps=30.000], 00:02:28.433 [frame=4453, fps=30.000]), (00:02:28.433 [frame=4453, fps=30.000], 00:02:33.167 [frame=4595, fps=30.000]), (00:02:33.167 [frame=4595, fps=30.000], 00:02:34.533 [frame=4636, fps=30.000]), (00:02:34.533 [frame=4636, fps=30.000], 00:02:37.667 [frame=4730, fps=30.000]), (00:02:37.667 [frame=4730, fps=30.000], 00:02:40.233 [frame=4807, fps=30.000]), (00:02:40.233 [frame=4807, fps=30.000], 00:02:41.233 [frame=4837, fps=30.000]), (00:02:41.233 [frame=4837, fps=30.000], 00:02:43.033 [frame=4891, fps=30.000]), (00:02:43.033 [frame=4891, fps=30.000], 00:02:44.933 [frame=4948, fps=30.000]), (00:02:44.933 [frame=4948, fps=30.000], 00:02:46.400 [frame=4992, fps=30.000]), (00:02:46.400 [frame=4992, fps=30.000], 00:02:47.667 [frame=5030, fps=30.000]), (00:02:47.667 [frame=5030, fps=30.000], 00:02:49.067 [frame=5072, fps=30.000]), (00:02:49.067 [frame=5072, fps=30.000], 00:02:50.267 [frame=5108, fps=30.000]), (00:02:50.267 [frame=5108, fps=30.000], 00:02:52.833 [frame=5185, fps=30.000]), (00:02:52.833 [frame=5185, fps=30.000], 00:02:54.833 [frame=5245, fps=30.000]), (00:02:54.833 [frame=5245, fps=30.000], 00:02:56.333 [frame=5290, fps=30.000]), (00:02:56.333 [frame=5290, fps=30.000], 00:02:57.333 [frame=5320, fps=30.000]), (00:02:57.333 [frame=5320, fps=30.000], 00:02:58.133 [frame=5344, fps=30.000]), (00:02:58.133 [frame=5344, fps=30.000], 00:02:58.633 [frame=5359, fps=30.000]), (00:02:58.633 [frame=5359, fps=30.000], 00:03:00.133 [frame=5404, fps=30.000]), (00:03:00.133 [frame=5404, fps=30.000], 00:03:01.533 [frame=5446, fps=30.000]), (00:03:01.533 [frame=5446, fps=30.000], 00:03:04.500 [frame=5535, fps=30.000])]\n",
      "[LOG] process_video: Processing scene #1 | Start: 0.00s, End: 3.13s.\n",
      "[LOG] trim_clip: Trimmed clip uploaded_video_001.mp4 from 0.0s to 3.1333333333333333s (max 4.0s).\n",
      "[LOG] extract_key_frame: Extracted key frame at 1.58s -> ..\\input\\frames\\uploaded_video_001_keyframe.jpg\n",
      "[LOG] speech_to_text: Checking audio format...\n",
      "    Channels: 1\n",
      "    Sample Width: 2\n",
      "    Frame Rate: 16000\n",
      "[LOG] speech_to_text: Transcribed 0 word(s).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Starting main pipeline process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m input_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m recognized_speech \u001b[38;5;241m=\u001b[39m speech_to_text(clip_path)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 5) Face detection & recognition on the key frame\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m face_identity \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_face_mediapipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 6) Object detection on the key frame\u001b[39;00m\n\u001b[0;32m     64\u001b[0m object_detections \u001b[38;5;241m=\u001b[39m detect_objects_yolo(frame_path)\n",
      "Cell \u001b[1;32mIn[7], line 110\u001b[0m, in \u001b[0;36mrecognize_face_mediapipe\u001b[1;34m(frame_path)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecognize_face_mediapipe\u001b[39m(frame_path):\n\u001b[1;32m--> 110\u001b[0m     mp_face_detection \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_detection\n\u001b[0;32m    111\u001b[0m     face_detection \u001b[38;5;241m=\u001b[39m mp_face_detection\u001b[38;5;241m.\u001b[39mFaceDetection(model_selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    113\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(frame_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_combined_text(record):\n",
    "    \"\"\"Combine key fields into one searchable text field.\"\"\"\n",
    "    parts = []\n",
    "    # Include face detection if present and not \"NoFaceDetected\"\n",
    "    face = record.get(\"face_detected\", \"\")\n",
    "    if face and face.lower() != \"nofacedetected\":\n",
    "        parts.append(face.replace(\"pins_\", \"\").lower())\n",
    "    # Include caption if available\n",
    "    caption = record.get(\"caption\", \"\")\n",
    "    if caption:\n",
    "        parts.append(caption.lower())\n",
    "    # Parse object_detection JSON and add object classes\n",
    "    try:\n",
    "        objects = json.loads(record.get(\"object_detection\", \"[]\"))\n",
    "        obj_names = [obj.get(\"class\", \"\").lower() for obj in objects if \"class\" in obj]\n",
    "        parts.extend(obj_names)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    # Include OCR text if available\n",
    "    ocr = record.get(\"ocr_text\", \"\")\n",
    "    if ocr:\n",
    "        parts.append(ocr.lower())\n",
    "    # Include action detection if not \"Unknown\"\n",
    "    action = record.get(\"action_detected\", \"\")\n",
    "    if action and action.lower() != \"unknown\":\n",
    "        parts.append(action.lower())\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# -------------------- Main Processing Pipeline --------------------\n",
    "def process_video(video_path):\n",
    "    print(f\"[LOG] process_video: Starting processing for '{video_path}'...\")\n",
    "    \n",
    "    # 1) Scene Detection\n",
    "    scenes = detect_scenes(video_path, content_threshold=10.0, threshold_val=20, min_scene_len=15)\n",
    "    print(f\"[DEBUG] process_video: Scenes -> {scenes}\")\n",
    "    if not scenes:\n",
    "        print(\"[DEBUG] process_video: No scenes detected! The CSV will likely be empty.\")\n",
    "    \n",
    "    results = []\n",
    "    # Ensure output directories exist (for clips and frames in ../input)\n",
    "    clips_dir = os.path.join(\"..\", \"input\", \"clips\")\n",
    "    frames_dir = os.path.join(\"..\", \"input\", \"frames\")\n",
    "    os.makedirs(clips_dir, exist_ok=True)\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, scene in enumerate(scenes, start=1):\n",
    "        start_sec = scene[0].get_seconds()\n",
    "        end_sec = scene[1].get_seconds()\n",
    "        print(f\"[LOG] process_video: Processing scene #{idx} | Start: {start_sec:.2f}s, End: {end_sec:.2f}s.\")\n",
    "        \n",
    "        # 2) Trim the scene, output to ../input/clips\n",
    "        clip_path = trim_clip(video_path, start_sec, end_sec, idx, max_duration=4.0, output_dir=clips_dir)\n",
    "        \n",
    "        # 3) Extract key frame, output to ../input/frames\n",
    "        frame_path = extract_key_frame(clip_path, frame_index='middle', output_dir=frames_dir)\n",
    "        \n",
    "        # 4) Speech-to-Text from the clip\n",
    "        recognized_speech = speech_to_text(clip_path)\n",
    "        \n",
    "        # 5) Face detection & recognition on the key frame\n",
    "        face_identity = recognize_face_mediapipe(frame_path)\n",
    "        \n",
    "        # 6) Object detection on the key frame\n",
    "        object_detections = detect_objects_yolo(frame_path)\n",
    "        \n",
    "        # 7) Action detection on the key frame\n",
    "    \n",
    "        \n",
    "        # 8) OCR to extract text\n",
    "        ocr_text = recognize_text(frame_path)\n",
    "        \n",
    "        # Collect results for this scene\n",
    "        record = {\n",
    "            \"clip_name\": os.path.basename(clip_path),\n",
    "            \"frame_name\": os.path.basename(frame_path) if frame_path else \"None\",\n",
    "            \"face_detected\": face_identity,\n",
    "            \"caption\": recognized_speech,\n",
    "            \"object_detection\": json.dumps(object_detections),\n",
    "            \"ocr_text\": ocr_text,\n",
    "            #add action detected here\n",
    "            \"timestamp_start\": start_sec,\n",
    "            \"timestamp_end\": end_sec\n",
    "        }\n",
    "        # Create the combined text for NLP search\n",
    "        record[\"combined_text\"] = create_combined_text(record)\n",
    "        \n",
    "        results.append(record)\n",
    "        print(f\"[LOG] process_video: Scene #{idx} processing complete.\\n\")\n",
    "    \n",
    "    print(f\"[DEBUG] process_video: Final results -> {results}\")\n",
    "    \n",
    "    # Create output directory for CSV\n",
    "    csv_dir = os.path.join(\"..\", \"output\", \"csv\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    csv_filename = os.path.join(csv_dir, \"final_results.csv\")\n",
    "    \n",
    "    # Write results to CSV, including the combined_text column\n",
    "    fieldnames = [\n",
    "        \"clip_name\", \"frame_name\", \"face_detected\", \"caption\",\n",
    "        \"object_detection\", \"ocr_text\", \"action_detected\",\n",
    "        \"timestamp_start\", \"timestamp_end\", \"combined_text\"\n",
    "    ]\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"[LOG] process_video: All scenes processed. Results saved to '{csv_filename}'.\")\n",
    "    print(\"Final Aggregated Results:\")\n",
    "    for record in results:\n",
    "        print(record)\n",
    "\n",
    "# -------------------- Main Guard --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[DEBUG] Starting main pipeline process...\")\n",
    "    input_video = \"../input/video.mp4\"\n",
    "    process_video(input_video)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videxact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
